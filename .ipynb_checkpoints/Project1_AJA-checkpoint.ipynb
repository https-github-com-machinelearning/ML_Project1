{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning -->\n",
    "# <center>Data Analysis and Machine Learning</center>\n",
    "<!-- dom:AUTHOR: Adriana Simancas, Aliaa Afify, Julgen Pellumaj>\n",
    "<!-- Author: -->  \n",
    "**Adriana Simancas, Aliaa Afify, Julgen Pellumaj**\n",
    "\n",
    "Date: **Feb 20, 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Project 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Classification and Regression, from linear and logistic regression to neural networks<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the project we will study the 1 dimensional (1D) case of the Ising model which is a mathematical model used to study and identify the phase transitions of a system  that consists of discrete variables that represent magnetic dipole moments of atomic \"spins\" that can be in one of two states (+1 or −1).  In the 1D case we can consider the system as a string in which the $N$-spins are “placed” with the condition that $s_{N} = s_{N+1}$. We can just imagine the string as ring and the spins placed on it pointing up or down. This system we will call it Ising state. \n",
    "\n",
    "The energy of an Ising state is given by the formula below:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    H = -J \\sum_{<sk>}^N s_k s_{k + 1},\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $s_{k} = ±1$ and $s_{l} = ±1$ are the spins of two neighbours, $N$ is the total number of spins, $J$ is a coupling constant expressing the strength of the interaction between neighboring spins. $< kl >$ indicates that we sum over nearest neighbors only.\n",
    "\n",
    "We are going to use a data set which contain the spin configuration of a given ising state and its corresponding energy which have been generated by assuming at first $J=1$. By using only the data set which contain the spins and the energies we are going to perform a linear fit and check the value of $J$.\n",
    "After we will generate the data which consist in a set of spins and energies, from which after performing a fit by using the linear regression method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing different packages\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Here we define the parameters of the Ising model\n",
    "# system size\n",
    "L=40  # the number of spins of an Ising state\n",
    "\n",
    "# creating 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "# we define a function to calculate the energies which correspond to different Ising states \n",
    "# we assume J=1 in order to get the data for the energy\n",
    "def ising_energies(states):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    L = states.shape[1]\n",
    "    J = np.zeros((L, L),)\n",
    "    for i in range(L): \n",
    "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors is assumed to be 1\n",
    "        \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states)\n",
    "####################################################\n",
    "\n",
    "# now we organize the data set composed from the spin configurations of the Ising states and the energy\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]\n",
    "####################################################\n",
    "\n",
    "# we split the data into training and test data\n",
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])\n",
    "#####################################################\n",
    "\n",
    "#importing packages\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# set up the linear regression model\n",
    "linreg=linear_model.LinearRegression()\n",
    "\n",
    "# define error lists for the training and test data\n",
    "train_errors_linreg = []\n",
    "test_errors_linreg = []\n",
    "\n",
    "#Initialize coeffficient\n",
    "coefs_linreg = []\n",
    "    \n",
    "# ordinary least squares\n",
    "linreg.fit(X_train, Y_train) # fit model \n",
    "coefs_linreg.append(linreg.coef_) # store weights\n",
    "# use the coefficient of determination R^2 as the performance of prediction.\n",
    "train_errors_linreg.append(linreg.score(X_train, Y_train))\n",
    "test_errors_linreg.append(linreg.score(X_test,Y_test))\n",
    "    \n",
    "# now we plot the results obtained from the fit\n",
    "# plot Ising interaction J\n",
    "J_linreg=np.array(linreg.coef_).reshape((L,L))\n",
    "\n",
    "cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "im = plt.imshow(J_linreg, **cmap_args)\n",
    "\n",
    "cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "    \n",
    "plt.imshow(J_linreg,**cmap_args)\n",
    "plt.title('Linear regression \\n Train$=%.3f$, Test$=%.3f$'%(train_errors_linreg[-1], test_errors_linreg[-1]),fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "    \n",
    "cbar=fig.colorbar(im)\n",
    "    \n",
    "cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments:$ Linear regression method gives a value of the coupling constant which is $J=0.597$ for the test data which is far from what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project we are going to fitt the same data set we had in $Part A$ by using linear regression, Ridge and Lasso methods. Each of the fitting methods will give a value of the coupling constant $J$ for the training and test data and the correspondig erros. By comparing these results we are going to conclude for the best fitting method in this case. The best method is going to be the one which gives a value of $J$ close to the real one. Ridge and Lasso method calculations have been done for different values of ${\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# set up linear, Lasso and Ridge Regression methods\n",
    "leastsq=linear_model.LinearRegression()\n",
    "ridge=linear_model.Ridge()\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "# define error lists\n",
    "train_errors_leastsq = []\n",
    "test_errors_leastsq = []\n",
    "\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "\n",
    "# set regularisation strength values\n",
    "lmbdas = np.logspace(-4, 5, 10)\n",
    "\n",
    "#Initialize coeffficients for ridge regression and Lasso\n",
    "coefs_leastsq = []\n",
    "coefs_ridge = []\n",
    "coefs_lasso=[]\n",
    "\n",
    "for lmbda in lmbdas:\n",
    "    \n",
    "    # ordinary least squares\n",
    "    leastsq.fit(X_train, Y_train) # fit model \n",
    "    coefs_leastsq.append(leastsq.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_leastsq.append(leastsq.score(X_train, Y_train))\n",
    "    test_errors_leastsq.append(leastsq.score(X_test,Y_test))\n",
    "    \n",
    "    # apply RIDGE regression\n",
    "    ridge.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    ridge.fit(X_train, Y_train) # fit model \n",
    "    coefs_ridge.append(ridge.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_ridge.append(ridge.score(X_train, Y_train))\n",
    "    test_errors_ridge.append(ridge.score(X_test,Y_test))\n",
    "    \n",
    "    # apply LASSO regression\n",
    "    lasso.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    lasso.fit(X_train, Y_train) # fit model\n",
    "    coefs_lasso.append(lasso.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_lasso.append(lasso.score(X_train, Y_train))\n",
    "    test_errors_lasso.append(lasso.score(X_test,Y_test))\n",
    "\n",
    "    # plot Ising interaction J\n",
    "    J_leastsq=np.array(leastsq.coef_).reshape((L,L))\n",
    "    J_ridge=np.array(ridge.coef_).reshape((L,L))\n",
    "    J_lasso=np.array(lasso.coef_).reshape((L,L))\n",
    "\n",
    "    cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "    \n",
    "    axarr[0].imshow(J_leastsq,**cmap_args)\n",
    "    axarr[0].set_title('OLS \\n Train$=%.3f$, Test$=%.3f$'%(train_errors_leastsq[-1], test_errors_leastsq[-1]),fontsize=16)\n",
    "    axarr[0].tick_params(labelsize=16)\n",
    "    \n",
    "    axarr[1].imshow(J_ridge,**cmap_args)\n",
    "    axarr[1].set_title('Ridge $\\lambda=%.4f$\\n Train$=%.3f$, Test$=%.3f$' %(lmbda,train_errors_ridge[-1],test_errors_ridge[-1]),fontsize=16)\n",
    "    axarr[1].tick_params(labelsize=16)\n",
    "    \n",
    "    im=axarr[2].imshow(J_lasso,**cmap_args)\n",
    "    axarr[2].set_title('LASSO $\\lambda=%.4f$\\n Train$=%.3f$, Test$=%.3f$' %(lmbda,train_errors_lasso[-1],test_errors_lasso[-1]),fontsize=16)\n",
    "    axarr[2].tick_params(labelsize=16)\n",
    "    \n",
    "    divider = make_axes_locatable(axarr[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, add_to_figure=True)\n",
    "    cbar=fig.colorbar(im, cax=cax)\n",
    "    \n",
    "    cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    cbar.set_label('$J_{i,j}$',labelpad=15, y=0.5,fontsize=20,rotation=0)\n",
    "    \n",
    "    fig.subplots_adjust(right=2.0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the performance of all the fitting methods by plotting all their results together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_errors_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_errors_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_errors_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_errors_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_errors_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_errors_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments:$\n",
    "We can see from the plot above that Lasso method gives a very good result for both the training and the test data. This method for the values of lambda, $10^{-3}\\leq{\\lambda}\\leq10^{-1}$ it gives values of $J$ which are very close to $1$.\n",
    "\n",
    "The two other methods doesn't give such good results. Ordinary least square method gives $J=0.597$ for the test data which is far from what we expect for the value of $J$.\n",
    "Ridge method also it gives values close to $0.6$, and even smaller for values of ${\\lambda}$ bigger than $10^{2}$.\n",
    "\n",
    "Lasso method is the best in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start working with the two dimensional case of the Ising model. From the theory is known that the $2D$ Ising model has a phase transition if the state is in a given temperature called 'critical temperature'. Ising states with temperature below the critical are called ordered states (spins point in one direction), and states with temperature above the critical are called disordered sates (spins point in different directions).\n",
    "In this part of the project by using binary classification methods and logistic regression we are going to train a model which will be able to predict the phase of a given sample when the spin configuration is known.\n",
    "\n",
    "To train the model a set of data which contains energies and spin configurations for several temperatures from Mehta et al has been used. We will use a fixed lattice of L × L = 40 × 40 spins in two dimensions. The theoretical critical temperature for a phase transition is TC ≈ 2.269 in units of energy. However, for a finite lattice the results representing the critical temperature are slightly higher (TC ≈ 2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed() # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from Mehta et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "from urllib.request import urlopen \n",
    "\n",
    "# url to data\n",
    "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
    "\n",
    "######### LOAD DATA\n",
    "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
    "# The labels are obtained from the following file:\n",
    "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "\n",
    "\n",
    "#DATA\n",
    "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this part of the code we split the data set into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can now visualize some of the data we have for the ordered, disordered and critical state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot a few Ising states\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# set colourbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimizing the cost function the are several minimization algorithms and methods. For our problem we are going to aply two different optimization routines, the liblinear (from scikit's logistic regression) and the stochastic gradient descent (SGD). In the end we are going to compare the performance of this two optimization models by taking into consideration the $accuracy$ of the classification model, which is the percentage of correctly classified data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "# loop over regularisation strength\n",
    "for i,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    # define logistic regressor\n",
    "    logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
    "                                           solver='liblinear')\n",
    "\n",
    "    # fit training data\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
    "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
    "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('accuracy: train, test, critical')\n",
    "    print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
    "\n",
    "    # define SGD-based logistic regression\n",
    "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
    "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
    "\n",
    "    # fit training data\n",
    "    logreg_SGD.fit(X_train,Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
    "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
    "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
    "\n",
    "    print('finished computing %i/11 iterations' %(i+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the accuracy of both of the models for the ordered, disordered and critical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy against regularisation strength\n",
    "plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
    "plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
    "plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
    "\n",
    "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
    "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
    "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n",
    "\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments$: From the plot above we can see the difference in accuracy when different optimization methods are aplied. The liblinear method has a better accuracy compared with the SGD. In the plot it can be clearly seen that the accuracy strongly depends on the regularization strength \"${\\lambda}$\" parameter. For the value of the regularization strength, ${\\lambda}$, around $10^{-1}$ the performance of both methods is approximately the same. There is also A difference in accuracy for the traing and test data set for both of the methods. Is very interesting to point out that for the model we have trained it become more difficult to predict the phase of the states which are close to criticality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project we will go back once again to the $1$ dimensional case of Ising model and our aim is to use now is to use scikit-learn in order to set up a neural network to find the optimal weights and biases. After we train the network we will compare the results with those obtained by linear regression code in $part A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code we generate the data in the same way we did in the part A of the project. By using \"$np.random.seed()$\" we are basically using the same data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "# Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40  # the number of spins in a string\n",
    "\n",
    "# create 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "def ising_energies(states):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    L = states.shape[1]\n",
    "    J = np.zeros((L, L),)\n",
    "    for i in range(L): \n",
    "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors\n",
    "        \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states)\n",
    "\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]\n",
    "\n",
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up here the neural network by using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_vals = np.logspace(-5,1,7)\n",
    "lmbd_vals = np.logspace(-5,1,7)\n",
    "n_hidden_neurons = 50\n",
    "epochs = 100\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# store models for later use\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_hidden_neurons), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# visual representation of grid search\n",
    "# uses seaborn heatmap, could probably do this in matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i in range(len(eta_vals)):\n",
    "    for j in range(len(lmbd_vals)):\n",
    "        dnn = DNN_scikit[i][j]\n",
    "        \n",
    "        train_pred = dnn.predict(X_train) \n",
    "        test_pred = dnn.predict(X_test)\n",
    "\n",
    "        train_accuracy[i][j] = accuracy_score(Y_train, train_pred)\n",
    "        test_accuracy[i][j] = accuracy_score(Y_test, test_pred)\n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments$: The model gives results with a very good accuracy for the learning rates values ${\\eta}=10^{-4},...,10^{-1}$ with different regularization parameters ${\\lambda}=10^{-5},...,10^{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come back again in the problem of classification of the different Ising states phases in ordered, disordered or critical. In the $Part C$ of the project we trained our model by using logistic regression, now in this part we are going to use neural network where the cost fonction change now to a log cross-entropy classification cost function. The results we obtain we are going to compare them with those obtained by using logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "import sys, os, argparse\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import dtypes\n",
    "# suppress tflow compilation warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "#tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a DataSet class and two functions read_data_sets and load_data to process the 2D Ising data.\n",
    "The DataSet class performs checks on the data shape and casts the data into the correct data type for the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, data_X, data_Y, dtype=dtypes.float32):\n",
    "        \"\"\"Checks data and casts it into correct data type. \"\"\"\n",
    "\n",
    "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "            raise TypeError('Invalid dtype %r, expected uint8 or float32' % dtype)\n",
    "\n",
    "        assert data_X.shape[0] == data_Y.shape[0], ('data_X.shape: %s data_Y.shape: %s' % (data_X.shape, data_Y.shape))\n",
    "        self.num_examples = data_X.shape[0]\n",
    "\n",
    "        if dtype == dtypes.float32:\n",
    "            data_X = data_X.astype(np.float32)\n",
    "        self.data_X = data_X\n",
    "        self.data_Y = data_Y \n",
    "\n",
    "        self.epochs_completed = 0\n",
    "        self.index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size, seed=None):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += batch_size\n",
    "        if self.index_in_epoch > self.num_examples:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self.data_X = self.data_X[perm]\n",
    "            self.data_Y = self.data_Y[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size\n",
    "            assert batch_size <= self.num_examples\n",
    "        end = self.index_in_epoch\n",
    "\n",
    "        return self.data_X[start:end], self.data_Y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load and split the data set it into three subsets: ordered, critical and disordered, depending on the temperature which sets the distribution they are drawn from. We use the ordered and disordered data to create a training and a test data set for the problem. The data in the critical phase are used in the end to test the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from urllib.request import urlopen \n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # path to data directory (for testing)\n",
    "    #path_to_data=os.path.expanduser('~')+'/Dropbox/MachineLearningReview/Datasets/isingMC/'\n",
    "\n",
    "    url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
    "\n",
    "    ######### LOAD DATA\n",
    "    # The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "    data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
    "    # The labels are obtained from the following file:\n",
    "    label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "\n",
    "    #DATA\n",
    "    data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "    data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "    data=data.astype('int')\n",
    "    data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "    #LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "    labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "    \n",
    "    print(\"Finished loading data\")\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def prepare_data(data, labels, dtype=dtypes.float32, test_size=0.2, validation_size=5000):\n",
    "    \n",
    "    L=40 # linear system size\n",
    "\n",
    "    # divide data into ordered, critical and disordered\n",
    "    X_ordered=data[:70000,:]\n",
    "    Y_ordered=labels[:70000]\n",
    "\n",
    "    X_critical=data[70000:100000,:]\n",
    "    Y_critical=labels[70000:100000]\n",
    "\n",
    "    X_disordered=data[100000:,:]\n",
    "    Y_disordered=labels[100000:]\n",
    "\n",
    "    # define training and test data sets\n",
    "    X=np.concatenate((X_ordered,X_disordered)) #np.concatenate((X_ordered,X_critical,X_disordered))\n",
    "    Y=np.concatenate((Y_ordered,Y_disordered)) #np.concatenate((Y_ordered,Y_critical,Y_disordered))\n",
    "\n",
    "    # pick random data points from ordered and disordered states to create the training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, train_size=1.0-test_size)\n",
    "\n",
    "    # make data categorical (i.e [0,1] or [1,0])\n",
    "    Y_train=to_categorical(Y_train)\n",
    "    Y_test=to_categorical(Y_test)\n",
    "    Y_critical=to_categorical(Y_critical)\n",
    "\n",
    "\n",
    "    if not 0 <= validation_size <= len(X_train):\n",
    "        raise ValueError('Validation size should be between 0 and {}. Received: {}.'.format(len(X_train), validation_size))\n",
    "\n",
    "    X_validation = X_train[:validation_size]\n",
    "    Y_validation = Y_train[:validation_size]\n",
    "    X_train = X_train[validation_size:]\n",
    "    Y_train = Y_train[validation_size:]\n",
    "\n",
    "    # create data sets\n",
    "    dataset = {\n",
    "        'train':DataSet(X_train, Y_train),\n",
    "        'test':DataSet(X_test, Y_test),\n",
    "        'critical':DataSet(X_critical, Y_critical),\n",
    "        'validation':DataSet(X_validation, Y_validation)\n",
    "    }\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The funciton deffined below is used to call the latter, one specifies the sizes for the training, test and validation data sets. This function also contains the local path to the file with the Ising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_Ising_DNN():\n",
    "    data, labels = load_data()\n",
    "    return prepare_data(data, labels, test_size=0.2, validation_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move on to construct our neural network using TensorFlow. To do this, we create a class called model. This class contains many useful function methods which break down the construction of the DNN. To classify whether a given spin configuration is in the ordered or disordered phase, we construct a minimalistic model for a DNN with a single hidden layer containing $N$ neurons \n",
    "(which is kept variable so we can try out the performance of different sizes for the hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    def __init__(self, N_neurons, opt_kwargs):\n",
    "        \"\"\"Builds the TFlow graph for the DNN.\n",
    "\n",
    "        N_neurons: number of neurons in the hidden layer\n",
    "        opt_kwargs: optimizer's arguments\n",
    "\n",
    "        \"\"\" \n",
    "\n",
    "        # define global step for checkpointing\n",
    "        self.global_step=tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.L=40 # system linear size\n",
    "        self.n_feats=self.L**2 # 40x40 square lattice\n",
    "        self.n_categories=2 # 2 Ising phases: ordered and disordered\n",
    "\n",
    "\n",
    "        # create placeholders for input X and label Y\n",
    "        self.create_placeholders()\n",
    "        # create weight and bias, initialized to 0 and construct DNN to predict Y from X\n",
    "        self.deep_layer_neurons=N_neurons\n",
    "        self.create_DNN()\n",
    "        # define loss function\n",
    "        self.create_loss()\n",
    "        # use gradient descent to minimize loss\n",
    "        self.create_optimiser(opt_kwargs)\n",
    "        #  create accuracy\n",
    "        self.create_accuracy()\n",
    "\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            # input layer\n",
    "            self.X=tf.placeholder(tf.float32, shape=(None, self.n_feats), name=\"X_data\")\n",
    "            # target\n",
    "            self.Y=tf.placeholder(tf.float32, shape=(None, self.n_categories), name=\"Y_data\")\n",
    "            # p\n",
    "            self.dropout_keepprob=tf.placeholder(tf.float32, name=\"keep_probability\")\n",
    "\n",
    "\n",
    "    def _weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "        # weights are drawn from a normal distribution with std 0.1 and mean 0.\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, dtype=dtype, name=name)\n",
    "\n",
    "\n",
    "    def _bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape) \n",
    "        return tf.Variable(initial, dtype=dtype, name=name)\n",
    "\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "\n",
    "            # Fully connected layer\n",
    "            W_fc1 = self._weight_variable([self.n_feats, self.deep_layer_neurons],name='fc1',dtype=tf.float32)\n",
    "            b_fc1 = self._bias_variable([self.deep_layer_neurons],name='fc1',dtype=tf.float32)\n",
    "\n",
    "            a_fc1 = tf.nn.relu(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "\n",
    "            # Softmax layer (see loss function)\n",
    "            W_fc2 = self._weight_variable([self.deep_layer_neurons, self.n_categories],name='fc2',dtype=tf.float32)\n",
    "            b_fc2 = self._bias_variable([self.n_categories],name='fc2',dtype=tf.float32)\n",
    "        \n",
    "            self.Y_predicted = tf.matmul(a_fc1, W_fc2) + b_fc2\n",
    "\n",
    "            \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y, logits=self.Y_predicted)\n",
    "                        )\n",
    "            # no need to use tf.stop_gradient() on labels because labels are placeholders and contain no params\n",
    "            # to be optimized. Backprop will be applied only to the logits. \n",
    "\n",
    "    def create_optimiser(self,opt_kwargs):\n",
    "        with tf.name_scope('optimiser'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(**opt_kwargs).minimize(self.loss,global_step=self.global_step) \n",
    "            #self.optimizer = tf.train.AdamOptimizer(**kwargs).minimize(self.loss,global_step=self.global_step)\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y, 1), tf.argmax(self.Y_predicted, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the performance of our model over a set of different learning rates, and a set of different hidden neurons. Therefore, we create a function evaluate_model which trains and evaluates the performance of our DNN for a fixed number of hidden neurons and a fixed SGD learning rate lr, and returns the final loss and accuracy for the three data sets of interest.\n",
    "Once we have exhausted all training epochs, we test the final performance on the entire training, test and critical data sets. \n",
    "In the end, we return the loss and accuracy for each of the training, test and critical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(neurons, lr, Ising_Data, verbose):\n",
    "    \"\"\"This function trains a DNN to solve the Ising classification problem\n",
    "\n",
    "    neurons: number of hidden neurons\n",
    "    lr: SGD learning rate\n",
    "    Ising_Data: Ising data set\n",
    "    verbose (bool): toggles output during the calculation \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    training_epochs=100\n",
    "    batch_size=100\n",
    "\n",
    "    # SGD learning params\n",
    "    opt_params=dict(learning_rate=lr)\n",
    "\n",
    "    # create DNN\n",
    "    DNN=model(neurons,opt_params)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # initialize the necessary variables, in this case, w and b\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train the DNN\n",
    "        for epoch in range(training_epochs): \n",
    "\n",
    "            batch_X, batch_Y = Ising_Data['train'].next_batch(batch_size,seed=seed)\n",
    "\n",
    "            loss_batch, _ = sess.run([DNN.loss,DNN.optimizer], \n",
    "                                    feed_dict={DNN.X: batch_X,\n",
    "                                               DNN.Y: batch_Y, \n",
    "                                               DNN.dropout_keepprob: 0.5} )\n",
    "            accuracy = sess.run(DNN.accuracy, \n",
    "                                feed_dict={DNN.X: batch_X,\n",
    "                                           DNN.Y: batch_Y, \n",
    "                                           DNN.dropout_keepprob: 1.0} )\n",
    "\n",
    "            # count training step\n",
    "            step = sess.run(DNN.global_step)\n",
    "\n",
    "\n",
    "        # test DNN performance on entire train test and critical data sets\n",
    "        train_loss, train_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['train'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['train'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 0.5}\n",
    "                                                                )\n",
    "        if verbose: print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
    "\n",
    "        test_loss, test_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['test'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['test'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "\n",
    "        if verbose: print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
    "\n",
    "        critical_loss, critical_accuracy = sess.run([DNN.loss, DNN.accuracy], \n",
    "                                                    feed_dict={DNN.X: Ising_Data['critical'].data_X,\n",
    "                                                               DNN.Y: Ising_Data['critical'].data_Y,\n",
    "                                                               DNN.dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "        if verbose: print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
    "\n",
    "\n",
    "        return train_loss,train_accuracy,test_loss,test_accuracy,critical_loss,critical_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the dependence of our DNN on some of the hyperparameters, we do a grid search over the number of neurons in the hidden layer, and different SGD learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(verbose):\n",
    "    \"\"\"This function performs a grid search over a set of different learning rates \n",
    "    and a number of hidden layer neurons.\"\"\"\n",
    "\n",
    "    # load Ising data\n",
    "    Ising_Data = prepare_Ising_DNN()\n",
    "    #Ising_Data=load_data()\n",
    "    \n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    N_neurons=np.logspace(0,3,4).astype('int') # check number of neurons over multiple decades\n",
    "    learning_rates=np.logspace(-6,-1,6)\n",
    "\n",
    "    # pre-alocate variables to store accuracy and loss data\n",
    "    train_loss=np.zeros((len(N_neurons),len(learning_rates)),dtype=np.float64)\n",
    "    train_accuracy=np.zeros_like(train_loss)\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "    critical_loss=np.zeros_like(train_loss)\n",
    "    critical_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, neurons in enumerate(N_neurons):\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "\n",
    "            print(\"training DNN with %4d neurons and SGD lr=%0.6f.\" %(neurons,lr) )\n",
    "\n",
    "            train_loss[i,j],train_accuracy[i,j],\\\n",
    "            test_loss[i,j],test_accuracy[i,j],\\\n",
    "            critical_loss[i,j],critical_accuracy[i,j] = evaluate_model(neurons,lr,Ising_Data,verbose)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,N_neurons,train_accuracy, 'training')\n",
    "    plot_data(learning_rates,N_neurons,test_accuracy, 'testing')\n",
    "    plot_data(learning_rates,N_neurons,critical_accuracy, 'critical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data,title=None):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,0.6,0.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code is going to run all the parts of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "grid_search(verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Coments$: Our neural network gives a very good performance for a number of hidden layers between $10$ and $1000$ where the learning rate is $lr=10^{-3},...,10^{-1}$. The accuracy of the model for these numbers of the hidden layer and for these values of the learning rate, for the test data is above $90%$. Is very interesting to point out that the model we have trained is also very good in classifing the data set which corresponding to the Ising states corresponding in the critical phase. Especially for the number of the hidden layers between $100$ and $1000$ and for a learning rate of $0.1$. The model in this case for the data set in the critical phase gives an accuracy on the classification between $90%$ and $95%$. With the other models we could not get such a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
