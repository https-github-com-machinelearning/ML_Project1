{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning -->\n",
    "# <center>Data Analysis and Machine Learning</center>\n",
    "<!-- dom:AUTHOR: Adriana Simancas de Filippo, Aliaa Afify, Julgen Pellumaj>\n",
    "<!-- Author: -->  \n",
    "**Adriana Simancas de Filippo, Aliaa Afify, Julgen Pellumaj**\n",
    "\n",
    "Date: **Feb 20, 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Project 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the project we will study the 1 dimensional (1D) case of the Ising model which is a mathematical model used to study and identify the phase transitions of a system  that consists of discrete variables that represent magnetic dipole moments of atomic \"spins\" that can be in one of two states (+1 or −1).  In the 1D case we can consider the system as a string in which the $N$-spins are “placed” with the condition that $s_{N} = s_{N+1}$. We can just imagine the string as ring and the spins placed on it pointing up or down. This system we will call it Ising state. \n",
    "\n",
    "The energy of an Ising state is given by the formula below:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    H = -J \\sum_{<sk>}^N s_k s_{k + 1},\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $s_{k} = ±1$ and $s_{l} = ±1$ are the spins of two neighbours, $N$ is the total number of spins, $J$ is a coupling constant expressing the strength of the interaction between neighboring spins. $< kl >$ indicates that we sum over nearest neighbors only.\n",
    "\n",
    "We are going to use a data set which contain the spin configuration of a given ising state and its corresponding energy which have been generated by assuming at first $J=1$. By using only the data set which contain the spins and the energies we are going to perform a linear fit and check the value of $J$.\n",
    "After we will generate the data which consist in a set of spins and energies, from which after performing a fit by using the linear regression method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing different packages\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Here we define the parameters of the Ising model\n",
    "# system size\n",
    "L=40  # the number of spins of an Ising state\n",
    "\n",
    "# creating 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "# we define a function to calculate the energies which correspond to different Ising states \n",
    "# we assume J=1 in order to get the data for the energy\n",
    "def ising_energies(states):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    L = states.shape[1]\n",
    "    J = np.zeros((L, L),)\n",
    "    for i in range(L): \n",
    "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors is assumed to be 1\n",
    "        \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states)\n",
    "####################################################\n",
    "\n",
    "# now we organize the data set composed from the spin configurations of the Ising states and the energy\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]\n",
    "####################################################\n",
    "\n",
    "# we split the data into training and test data\n",
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])\n",
    "#####################################################\n",
    "\n",
    "#importing packages\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# set up the linear regression model\n",
    "linreg=linear_model.LinearRegression()\n",
    "\n",
    "# define error lists for the training and test data\n",
    "train_errors_linreg = []\n",
    "test_errors_linreg = []\n",
    "\n",
    "#Initialize coeffficient\n",
    "coefs_linreg = []\n",
    "    \n",
    "# ordinary least squares\n",
    "linreg.fit(X_train, Y_train) # fit model \n",
    "coefs_linreg.append(linreg.coef_) # store weights\n",
    "# use the coefficient of determination R^2 as the performance of prediction.\n",
    "train_errors_linreg.append(linreg.score(X_train, Y_train))\n",
    "test_errors_linreg.append(linreg.score(X_test,Y_test))\n",
    "    \n",
    "# now we plot the results obtained from the fit\n",
    "# plot Ising interaction J\n",
    "J_linreg=np.array(linreg.coef_).reshape((L,L))\n",
    "\n",
    "cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "im = plt.imshow(J_linreg, **cmap_args)\n",
    "\n",
    "cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "    \n",
    "plt.imshow(J_linreg,**cmap_args)\n",
    "plt.title('Linear regression \\n Train$=%.3f$, Test$=%.3f$'%(train_errors_linreg[-1], test_errors_linreg[-1]),fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "    \n",
    "cbar=fig.colorbar(im)\n",
    "    \n",
    "cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments:$ Linear regression method gives a value of the coupling constant which is $J=0.597$ for the test data which is far from what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project we are going to fitt the same data set we had in $Part A$ by using linear regression, Ridge and Lasso methods. Each of the fitting methods will give a value of the coupling constant $J$ for the training and test data and the correspondig erros. By comparing these results we are going to conclude for the best fitting method in this case. The best method is going to be the one which gives a value of $J$ close to the real one. Ridge and Lasso method calculations have been done for different values of ${\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# set up linear, Lasso and Ridge Regression methods\n",
    "leastsq=linear_model.LinearRegression()\n",
    "ridge=linear_model.Ridge()\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "# define error lists\n",
    "train_errors_leastsq = []\n",
    "test_errors_leastsq = []\n",
    "\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "\n",
    "# set regularisation strength values\n",
    "lmbdas = np.logspace(-4, 5, 10)\n",
    "\n",
    "#Initialize coeffficients for ridge regression and Lasso\n",
    "coefs_leastsq = []\n",
    "coefs_ridge = []\n",
    "coefs_lasso=[]\n",
    "\n",
    "for lmbda in lmbdas:\n",
    "    \n",
    "    # ordinary least squares\n",
    "    leastsq.fit(X_train, Y_train) # fit model \n",
    "    coefs_leastsq.append(leastsq.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_leastsq.append(leastsq.score(X_train, Y_train))\n",
    "    test_errors_leastsq.append(leastsq.score(X_test,Y_test))\n",
    "    \n",
    "    # apply RIDGE regression\n",
    "    ridge.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    ridge.fit(X_train, Y_train) # fit model \n",
    "    coefs_ridge.append(ridge.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_ridge.append(ridge.score(X_train, Y_train))\n",
    "    test_errors_ridge.append(ridge.score(X_test,Y_test))\n",
    "    \n",
    "    # apply LASSO regression\n",
    "    lasso.set_params(alpha=lmbda) # set regularisation parameter\n",
    "    lasso.fit(X_train, Y_train) # fit model\n",
    "    coefs_lasso.append(lasso.coef_) # store weights\n",
    "    # use the coefficient of determination R^2 as the performance of prediction.\n",
    "    train_errors_lasso.append(lasso.score(X_train, Y_train))\n",
    "    test_errors_lasso.append(lasso.score(X_test,Y_test))\n",
    "\n",
    "    # plot Ising interaction J\n",
    "    J_leastsq=np.array(leastsq.coef_).reshape((L,L))\n",
    "    J_ridge=np.array(ridge.coef_).reshape((L,L))\n",
    "    J_lasso=np.array(lasso.coef_).reshape((L,L))\n",
    "\n",
    "    cmap_args=dict(vmin=-1., vmax=1., cmap='seismic')\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "    \n",
    "    axarr[0].imshow(J_leastsq,**cmap_args)\n",
    "    axarr[0].set_title('OLS \\n Train$=%.3f$, Test$=%.3f$'%(train_errors_leastsq[-1], test_errors_leastsq[-1]),fontsize=16)\n",
    "    axarr[0].tick_params(labelsize=16)\n",
    "    \n",
    "    axarr[1].imshow(J_ridge,**cmap_args)\n",
    "    axarr[1].set_title('Ridge $\\lambda=%.4f$\\n Train$=%.3f$, Test$=%.3f$' %(lmbda,train_errors_ridge[-1],test_errors_ridge[-1]),fontsize=16)\n",
    "    axarr[1].tick_params(labelsize=16)\n",
    "    \n",
    "    im=axarr[2].imshow(J_lasso,**cmap_args)\n",
    "    axarr[2].set_title('LASSO $\\lambda=%.4f$\\n Train$=%.3f$, Test$=%.3f$' %(lmbda,train_errors_lasso[-1],test_errors_lasso[-1]),fontsize=16)\n",
    "    axarr[2].tick_params(labelsize=16)\n",
    "    \n",
    "    divider = make_axes_locatable(axarr[2])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, add_to_figure=True)\n",
    "    cbar=fig.colorbar(im, cax=cax)\n",
    "    \n",
    "    cbar.ax.set_yticklabels(np.arange(-1.0, 1.0+0.25, 0.25),fontsize=14)\n",
    "    cbar.set_label('$J_{i,j}$',labelpad=15, y=0.5,fontsize=20,rotation=0)\n",
    "    \n",
    "    fig.subplots_adjust(right=2.0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the performance of all the fitting methods by plotting all their results together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our performance on both the training and test data\n",
    "plt.semilogx(lmbdas, train_errors_leastsq, 'b',label='Train (OLS)')\n",
    "plt.semilogx(lmbdas, test_errors_leastsq,'--b',label='Test (OLS)')\n",
    "plt.semilogx(lmbdas, train_errors_ridge,'r',label='Train (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, test_errors_ridge,'--r',label='Test (Ridge)',linewidth=1)\n",
    "plt.semilogx(lmbdas, train_errors_lasso, 'g',label='Train (LASSO)')\n",
    "plt.semilogx(lmbdas, test_errors_lasso, '--g',label='Test (LASSO)')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "#           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlim([min(lmbdas), max(lmbdas)])\n",
    "plt.xlabel(r'$\\lambda$',fontsize=16)\n",
    "plt.ylabel('Performance',fontsize=16)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments:$\n",
    "We can see from the plot above that Lasso method gives a very good result for both the training and the test data. This method for the values of lambda, $10^{-3}\\leq{\\lambda}\\leq10^{-1}$ it gives values of $J$ which are very close to $1$.\n",
    "\n",
    "The two other methods doesn't give such good results. Ordinary least square method gives $J=0.597$ for the test data which is far from what we expect for the value of $J$.\n",
    "Ridge method also it gives values close to $0.6$, and even smaller for values of ${\\lambda}$ bigger than $10^{2}$.\n",
    "\n",
    "Lasso method is the best in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start working with the two dimensional case of the Ising model. From the theory is known that the $2D$ Ising model has a phase transition if the state is in a given temperature called 'critical temperature'. Ising states with temperature below the critical are called ordered states (spins point in one direction), and states with temperature above the critical are called disordered sates (spins point in different directions).\n",
    "In this part of the project by using binary classification methods and logistic regression we are going to train a model which will be able to predict the phase of a given sample when the spin configuration is known.\n",
    "\n",
    "To train the model a set of data which contains energies and spin configurations for several temperatures from Mehta et al has been used. We will use a fixed lattice of L × L = 40 × 40 spins in two dimensions. The theoretical critical temperature for a phase transition is TC ≈ 2.269 in units of energy. However, for a finite lattice the results representing the critical temperature are slightly higher (TC ≈ 2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed() # shuffle random seed generator\n",
    "\n",
    "# Ising model parameters\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "T_c=2.26 # Onsager critical temperature in the TD limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from Mehta et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "from urllib.request import urlopen \n",
    "\n",
    "# url to data\n",
    "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
    "\n",
    "######### LOAD DATA\n",
    "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
    "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
    "# The labels are obtained from the following file:\n",
    "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "\n",
    "\n",
    "#DATA\n",
    "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
    "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this part of the code we split the data set into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### define ML parameters\n",
    "num_classes=2\n",
    "train_to_test_ratio=0.5 # training samples\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:70000,:]\n",
    "Y_ordered=labels[:70000]\n",
    "\n",
    "X_critical=data[70000:100000,:]\n",
    "Y_critical=labels[70000:100000]\n",
    "\n",
    "X_disordered=data[100000:,:]\n",
    "Y_disordered=labels[100000:]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered))\n",
    "\n",
    "# pick random data points from ordered and disordered states \n",
    "# to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "# full data set\n",
    "X=np.concatenate((X_critical,X))\n",
    "Y=np.concatenate((Y_critical,Y))\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_critical.shape[0], 'critical samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can now visualize some of the data we have for the ordered, disordered and critical state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot a few Ising states\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# set colourbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "# plot states\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimizing the cost function the are several minimization algorithms and methods. For our problem we are going to aply two different optimization routines, the liblinear (from scikit's logistic regression) and the stochastic gradient descent (SGD). In the end we are going to compare the performance of this two optimization models by taking into consideration the $accuracy$ of the classification model, which is the percentage of correctly classified data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logistic regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# define regularisation parameter\n",
    "lmbdas=np.logspace(-5,5,11)\n",
    "\n",
    "# preallocate data\n",
    "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
    "\n",
    "# loop over regularisation strength\n",
    "for i,lmbda in enumerate(lmbdas):\n",
    "\n",
    "    # define logistic regressor\n",
    "    logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
    "                                           solver='liblinear')\n",
    "\n",
    "    # fit training data\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
    "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
    "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('accuracy: train, test, critical')\n",
    "    print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
    "\n",
    "    # define SGD-based logistic regression\n",
    "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
    "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
    "\n",
    "    # fit training data\n",
    "    logreg_SGD.fit(X_train,Y_train)\n",
    "\n",
    "    # check accuracy\n",
    "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
    "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
    "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
    "    \n",
    "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
    "\n",
    "    print('finished computing %i/11 iterations' %(i+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the accuracy of both of the models for the ordered, disordered and critical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy against regularisation strength\n",
    "plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
    "plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
    "plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
    "\n",
    "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
    "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
    "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n",
    "\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Comments$: From the plot above we can see the difference in accuracy when different optimization methods are aplied. The liblinear method has a better accuracy compared with the SGD. In the plot it can be clearly seen that the accuracy strongly depends on the regularization strength \"${\\lambda}$\" parameter. For the value of the regularization strength, ${\\lambda}$, around $10^{-1}$ the performance of both methods is approximately the same. There is also A difference in accuracy for the traing and test data set for both of the methods. Is very interesting to point out that for the model we have trained it become more difficult to predict the phase of the states which are close to criticality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "# Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40  # the number of spins in a string\n",
    "\n",
    "# create 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(10000,L))\n",
    "\n",
    "def ising_energies(states):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    L = states.shape[1]\n",
    "    J = np.zeros((L, L),)\n",
    "    for i in range(L): \n",
    "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors\n",
    "        \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states)\n",
    "####################################################\n",
    "\n",
    "\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2]))\n",
    "# build final data set\n",
    "Data=[states,energies]\n",
    "####################################################\n",
    "\n",
    "# define number of samples\n",
    "n_samples=400\n",
    "# define train and test data sets\n",
    "X_train=Data[0][:n_samples]\n",
    "Y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])\n",
    "X_test=Data[0][n_samples:3*n_samples//2]\n",
    "Y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_vals = np.logspace(-5,1,7)\n",
    "lmbd_vals = np.logspace(-5,1,7)\n",
    "n_hidden_neurons = 50\n",
    "epochs = 100\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# store models for later use\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_hidden_neurons), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# visual representation of grid search\n",
    "# uses seaborn heatmap, could probably do this in matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i in range(len(eta_vals)):\n",
    "    for j in range(len(lmbd_vals)):\n",
    "        dnn = DNN_scikit[i][j]\n",
    "        \n",
    "        train_pred = dnn.predict(X_train) \n",
    "        test_pred = dnn.predict(X_test)\n",
    "\n",
    "        train_accuracy[i][j] = accuracy_score(Y_train, train_pred)\n",
    "        test_accuracy[i][j] = accuracy_score(Y_test, test_pred)\n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"$\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
